import os
import sys
from datetime import datetime, timedelta
import numpy as np
import ffmpeg
import torch
from scipy.signal import find_peaks
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ PyTorch-–≤–µ—Ä—Å–∏–∏ TransNetV2
sys.path.append('TransNetV2/inference-pytorch')
from transnetv2_pytorch import TransNetV2

# --- 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
VIDEO_PATH = os.path.join('videos_processed', 'video_compilation_021.mp4')
CHAPTERS_PATH = os.path.join('videos_chapters', 'video_compilation_021.txt')
PYTORCH_WEIGHTS_PATH = os.path.join('TransNetV2', 'inference-pytorch', 'transnetv2-pytorch-weights.pth')
# –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
OUTPUT_DIR = 'transnet_results'
TOLERANCE_SECONDS = 1.0
BATCH_SIZE = 1000

# --- 2. –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---
def parse_chapters_file(file_path: str) -> list[float]:
    timestamps_sec = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split(' ')
                if len(parts) < 2: continue
                time_str = parts[1]
                main_time, ms_part = time_str.rsplit(':', 1)
                dt_obj = datetime.strptime(main_time, '%H:%M:%S')
                total_seconds = (dt_obj.hour * 3600 + dt_obj.minute * 60 + dt_obj.second + int(ms_part) / 100.0)
                timestamps_sec.append(total_seconds)
    except FileNotFoundError:
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª —Å –≥–ª–∞–≤–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return []
    return [ts for ts in timestamps_sec if ts > 0]

def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.5) -> np.ndarray:
    peaks, _ = find_peaks(predictions, height=threshold, distance=1)
    if len(peaks) == 0:
        return np.array([[0, len(predictions) - 1]], dtype=np.int32)

    scene_boundaries = np.concatenate(([0], peaks, [len(predictions) - 1])).astype(np.int32)
    unique_boundaries = sorted(list(set(scene_boundaries)))
    
    scenes = np.array([
        [unique_boundaries[i], unique_boundaries[i+1] -1]
        for i in range(len(unique_boundaries) - 1)
    ], dtype=np.int32)
    
    return scenes

# ‚úÖ –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
def format_seconds(seconds: float) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–µ–∫—É–Ω–¥—ã –≤ —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ HH:MM:SS:ms."""
    td = timedelta(seconds=seconds)
    minutes, sec = divmod(td.seconds, 60)
    hours, minutes = divmod(minutes, 60)
    milliseconds = int(td.microseconds / 10000) # 2 –∑–Ω–∞–∫–∞ –¥–ª—è ms
    return f"{hours:02d}:{minutes:02d}:{sec:02d}:{milliseconds:02d}"

# --- 3. –û–°–ù–û–í–ù–û–ô –°–ö–†–ò–ü–¢ ---
def main():
    if not os.path.exists(VIDEO_PATH):
        print(f"–û—à–∏–±–∫–∞: –í–∏–¥–µ–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {VIDEO_PATH}")
        return
    if not os.path.exists(PYTORCH_WEIGHTS_PATH):
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª —Å –≤–µ—Å–∞–º–∏ PyTorch –Ω–µ –Ω–∞–π–¥–µ–Ω: {PYTORCH_WEIGHTS_PATH}")
        return

    # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–ò–ï: –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
    print(f"üìñ –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ñ–∞–π–ª –≤–∏–¥–µ–æ:    {VIDEO_PATH}")
    print(f"üìñ –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ñ–∞–π–ª –≥–ª–∞–≤: {CHAPTERS_PATH}")

    print("\nüîç –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∏–¥–µ–æ...")
    try:
        probe = ffmpeg.probe(VIDEO_PATH)
        video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')
        framerate_str = video_info['r_frame_rate'].split('/')
        framerate = float(framerate_str[0]) / float(framerate_str[1])
        num_frames = int(video_info['nb_frames'])
        print(f"–í–∏–¥–µ–æ: {video_info['width']}x{video_info['height']}, {num_frames} –∫–∞–¥—Ä–æ–≤, {framerate:.2f} fps.")
    except Exception as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ: {e}")
        return

    print("\nüöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyTorch-–º–æ–¥–µ–ª–∏ TransNetV2...")
    model = TransNetV2()
    model.load_state_dict(torch.load(PYTORCH_WEIGHTS_PATH))
    model.eval().cuda()

    print("\nüöÄ –ù–∞—á–∞–ª–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ —Å GPU-—É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
    
    TARGET_WIDTH, TARGET_HEIGHT = 48, 27
    args = (
        ffmpeg
        .input(VIDEO_PATH, hwaccel='cuda', vsync=0)
        .filter('scale', width=TARGET_WIDTH, height=TARGET_HEIGHT)
        .output('pipe:', format='rawvideo', pix_fmt='rgb24')
        .global_args('-hide_banner', '-loglevel', 'error')
    )
    process = ffmpeg.run_async(args, pipe_stdout=True)

    all_predictions = []
    
    with tqdm(total=num_frames, unit="frames", desc="Processing video") as pbar:
        with torch.no_grad():
            while True:
                in_bytes = process.stdout.read(TARGET_WIDTH * TARGET_HEIGHT * 3 * BATCH_SIZE)
                if not in_bytes: break
                
                in_frames = np.frombuffer(in_bytes, np.uint8).reshape([-1, TARGET_HEIGHT, TARGET_WIDTH, 3]).copy()
                in_frames_tensor = torch.from_numpy(in_frames).cuda().to(torch.uint8)
                single_frame_pred, _ = model(in_frames_tensor.unsqueeze(0))
                
                predictions = torch.sigmoid(single_frame_pred).cpu().numpy()
                all_predictions.append(predictions)
                pbar.update(len(in_frames))

    process.wait()
    print("\n‚úÖ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω—ã.")
    
    final_predictions = np.concatenate(all_predictions, axis=1).flatten()
    
    scenes = predictions_to_scenes(final_predictions)
    detected_frames = [scene[0] for scene in scenes if scene[0] > 0]
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(detected_frames)} –ø–µ—Ä–µ—Ö–æ–¥–æ–≤.")

    # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª
    base_name = os.path.splitext(os.path.basename(VIDEO_PATH))[0]
    output_path = os.path.join(OUTPUT_DIR, f"{base_name}.txt")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        for i, frame_num in enumerate(detected_frames, 1):
            timestamp_sec = frame_num / framerate
            time_str = format_seconds(timestamp_sec)
            line = f"{i:07d} {time_str} {frame_num}\n"
            f.write(line)
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_path}")

    print("\nüìä –ù–∞—á–∞–ª–æ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    ground_truth_times = parse_chapters_file(CHAPTERS_PATH)
    detected_times = [frame / framerate for frame in detected_frames]

    if not ground_truth_times or not detected_times:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –∏–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏. –û—Ü–µ–Ω–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
        return

    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ground_truth_times)} –∏—Å—Ç–∏–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤.")
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(detected_times)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤.")

    true_positives, false_negatives = 0, 0
    temp_detected_times = list(detected_times)
    for gt_time in ground_truth_times:
        found_match, best_match, min_diff = False, -1, float('inf')
        for det_time in temp_detected_times:
            diff = abs(gt_time - det_time)
            if diff <= TOLERANCE_SECONDS and diff < min_diff:
                min_diff, best_match, found_match = diff, det_time, True
        if found_match:
            true_positives += 1
            temp_detected_times.remove(best_match)
        else: false_negatives += 1
            
    false_positives = len(detected_times) - true_positives
    recall = true_positives / len(ground_truth_times) if ground_truth_times else 0.0
    precision = true_positives / len(detected_times) if detected_times else 0.0

    print("\n--- [–û–¢–ß–ï–¢ TransNetV2 c PyTorch] ---")
    print(f"–ù–∞–π–¥–µ–Ω–æ –≤–µ—Ä–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ (True Positives): {true_positives}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ (False Negatives): {false_negatives}")
    print(f"–õ–∏—à–Ω–∏–µ (–ª–æ–∂–Ω—ã–µ) –ø–µ—Ä–µ—Ö–æ–¥—ã (False Positives): {false_positives}")
    print("-------------------------")
    print(f"üéØ –ü–æ–ª–Ω–æ—Ç–∞ (Recall):    {recall:.2%}")
    print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å (Precision): {precision:.2%}")
    print("-------------------------")

if __name__ == '__main__':
    main()